{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0CTSL0o5Gaum0u+Rb+JtZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcivanmanalac/CNN_Preprocessor/blob/main/marcs_CNN_RESNET50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Google Drive.\n"
      ],
      "metadata": {
        "id": "1mvBXA49jGis"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9KRGxx8wi520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616ebbe6-4ce0-45cd-d0ba-b21ec4937dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if the dataset is there"
      ],
      "metadata": {
        "id": "tRLWIywVSEIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/drive/MyDrive/labeled_images_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzUmFYbOSF_l",
        "outputId": "aae72d64-7ca4-4f56-ea6e-f426bb5e9959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3740\n",
            "-rw------- 1 root root 1901787 Feb 21 22:33 annotations.csv\n",
            "drwx------ 2 root root    4096 Feb 20 06:57 East_Waiawa_Plots\n",
            "drwx------ 2 root root    4096 Feb 20 06:57 East_Waiawa_Plots_Original\n",
            "drwx------ 2 root root    4096 Feb 20 06:57 Loi_Behind_LLC\n",
            "drwx------ 2 root root    4096 Feb 20 06:57 Loi_Behind_LLC_Original\n",
            "-rw------- 1 root root  190286 Feb 21 22:33 test.csv\n",
            "-rw------- 1 root root     178 Feb 21 22:34 test.gsheet\n",
            "-rw------- 1 root root 1140763 Feb 21 22:33 train.csv\n",
            "-rw------- 1 root root  570820 Feb 21 22:33 val.csv\n",
            "drwx------ 2 root root    4096 Feb 20 06:57 West_Waiawa_Plots\n",
            "drwx------ 2 root root    4096 Feb 20 06:57 West_Waiawa_Plots_Original\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this Cell to update XML Paths for new labeled images\n"
      ],
      "metadata": {
        "id": "wovK2somKp7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from xml.etree import ElementTree as ET\n",
        "\n",
        "def batch_edit_xml(xml_directory, jpg_directory):\n",
        "    count = 1 # initializing count to 1\n",
        "    for root, dirs, files in os.walk(xml_directory):\n",
        "        for file in files:\n",
        "            if file.endswith(\".xml\"):\n",
        "                file_path = os.path.join(root, file) # creating a file path by joining the root and the file name\n",
        "                xml_tree = ET.parse(file_path) # parsing the XML file\n",
        "                xml_root = xml_tree.getroot() # getting the root of the XML file\n",
        "                filename = os.path.splitext(file)[0] # getting the file name without the extension\n",
        "                jpg_path = os.path.join(jpg_directory, os.path.basename(root), filename + '.jpg').replace(os.sep, '/') # creating a jpg path and converting to forward slashes\n",
        "                xml_root.find('./path').text = jpg_path # finding the path element in the XML file and updating it with the jpg_path\n",
        "                xml_tree.write(file_path) # writing the changes back to the XML file\n",
        "                print(f\"{count} of {len(files)}: {file_path}\") # printing the current count and the total number of files processed\n",
        "                count += 1\n",
        "                if count > len(files): # checking if the count has reached the length of the files\n",
        "                    count = 1 # resetting the count back to 1\n",
        "    print(\"Edit Complete\") # indicating that the edit is complete\n",
        "\n",
        "def main():\n",
        "    ew_xml_dir = '/content/drive/MyDrive/labeled_images_dataset/East_Waiawa_Plots/East_Waiawa_Plots_XML/'\n",
        "    ew_jpg_dir = '/content/drive/MyDrive/labeled_images_dataset/East_Waiawa_Plots/East_Waiawa_Plots_Cropped/'\n",
        "    ww_xml_dir = '/content/drive/MyDrive/labeled_images_dataset/West_Waiawa_Plots/West_Waiawa_Plots_XML/'\n",
        "    ww_jpg_dir = '/content/drive/MyDrive/labeled_images_dataset/West_Waiawa_Plots/West_Waiawa_Plots_Cropped/'\n",
        "    loi_xml_dir = '/content/drive/MyDrive/labeled_images_dataset/Loi_Behind_LLC/Loi_Behind_LLC_XML/'\n",
        "    loi_jpg_dir = '/content/drive/MyDrive/labeled_images_dataset/Loi_Behind_LLC/Loi_Behind_LLC_Cropped'\n",
        "    batch_edit_xml(ew_xml_dir,ew_jpg_dir)\n",
        "    batch_edit_xml(ww_xml_dir,ww_jpg_dir)\n",
        "    batch_edit_xml(loi_xml_dir,loi_jpg_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "yCf8oTpXLA6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Annotations CSV"
      ],
      "metadata": {
        "id": "U7y2IZ9xTxfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import csv\n",
        "import os\n",
        "import tkinter as tk\n",
        "from tkinter import filedialog\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "\n",
        "def get_dirs():\n",
        "    # Define XML Root Directory and CSV Output Directory\n",
        "    xml_dir = '/content/drive/MyDrive/labeled_images_dataset/'\n",
        "    csv_dir = '/content/drive/MyDrive/labeled_images_dataset/'\n",
        "    class_names = input(\"Enter class names separated by commas: \").split(\",\")\n",
        "    labels = input(\"Enter labels separated by commas: \").split(\",\")\n",
        "    return xml_dir, csv_dir, class_names, labels\n",
        "\n",
        "\n",
        "def process_xml_file(xml_file, class_names, labels):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    data = []\n",
        "    for obj in root.findall(\"object\"):\n",
        "        name = obj.find(\"name\").text\n",
        "        if name in class_names:\n",
        "            label = labels[class_names.index(name)].strip() # strip spaces from the label value\n",
        "            bndbox = obj.find(\"bndbox\")\n",
        "            xmin = int(bndbox.find(\"xmin\").text)\n",
        "            ymin = int(bndbox.find(\"ymin\").text)\n",
        "            xmax = int(bndbox.find(\"xmax\").text)\n",
        "            ymax = int(bndbox.find(\"ymax\").text)\n",
        "            path = root.find(\"path\").text  # get the path tag\n",
        "            #trying to fix CSV output\n",
        "            #data.append((path, xmin, ymin, xmax, ymax, label))\n",
        "            data.append((path, xmin, ymin, xmax, ymax, label))\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def process_xml_files(xml_dir, csv_dir, class_names, labels):\n",
        "    csv_file = os.path.join(csv_dir, \"annotations.csv\")\n",
        "    with open(csv_file, \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"filename\", \"path\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"label\"])  # add \"path\" column\n",
        "        for root, dirs, files in os.walk(xml_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(\".xml\"):\n",
        "                    xml_file = os.path.join(root, file)\n",
        "                    data = process_xml_file(xml_file, class_names, labels)\n",
        "                    if data:\n",
        "                        filename = os.path.splitext(file)[0] + \".jpg\"\n",
        "                        for row in data:\n",
        "                            #EDIT\n",
        "                            #writer.writerow(list(row))  # include path value as first element\n",
        "                            writer.writerow([filename] + list(row))\n",
        "def main():\n",
        "    xml_dir, csv_dir, class_names, labels = get_dirs()\n",
        "    process_xml_files(xml_dir, csv_dir, class_names, labels)\n",
        "    print(\"CSV file saved to: \", os.path.join(csv_dir, \"annotations.csv\"))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3TxyuWuT1bB",
        "outputId": "29e1c6f0-1ba0-4aef-f84a-9780f2c48d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter class names separated by commas: notch,infested_frond,infested_tree,healthy_frond,healthy_tree\n",
            "Enter labels separated by commas: notch,infested_frond,infested_tree,healthy_frond,healthy_tree\n",
            "CSV file saved to:  /content/drive/MyDrive/labeled_images_dataset/annotations.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the created CSV file"
      ],
      "metadata": {
        "id": "2lUsjASY1XRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "csv_path = \"/content/drive/MyDrive/labeled_images_dataset/annotations.csv\"  # Replace with the path to your CSV file\n",
        "df = pd.read_csv(csv_path)\n",
        "# Count the number of rows not including the header\n",
        "num_rows = df.shape[0] - 1\n",
        "\n",
        "# Print the number of rows\n",
        "print(\"Lable Count: \", num_rows)\n",
        "print(df.head(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViQhJjGF1qpT",
        "outputId": "10fbc6f3-1dc5-4f56-ddc6-eb14d59b5b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lable Count:  13529\n",
            "  filename                                               path  xmin  ymin  \\\n",
            "0   32.jpg  /content/drive/MyDrive/labeled_images_dataset/...   605   134   \n",
            "1   32.jpg  /content/drive/MyDrive/labeled_images_dataset/...   528    90   \n",
            "2   32.jpg  /content/drive/MyDrive/labeled_images_dataset/...   511    66   \n",
            "\n",
            "   xmax  ymax           label  \n",
            "0   635   165           notch  \n",
            "1   596   129           notch  \n",
            "2   651   193  infested_frond  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the Annotations CSV to create training, validation, & test CSV files\n",
        "\n"
      ],
      "metadata": {
        "id": "hNGpo_t4pKon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Enter input CSV File Path and Destination\n",
        "input_file_path = '/content/drive/MyDrive/labeled_images_dataset/annotations.csv'\n",
        "output_dir_path = '/content/drive/MyDrive/labeled_images_dataset/'\n",
        "\n",
        "# Path to the output CSV files for the training, validation, and testing sets\n",
        "train_csv_file = os.path.join(output_dir_path, 'train.csv')\n",
        "val_csv_file = os.path.join(output_dir_path, 'val.csv')\n",
        "test_csv_file = os.path.join(output_dir_path, 'test.csv')\n",
        "\n",
        "# Proportions of the split (train: 60%, val: 30%, test: 10%)\n",
        "train_split = 0.6\n",
        "val_split = 0.3\n",
        "test_split = 0.1\n",
        "\n",
        "# Read the CSV file and split the rows randomly\n",
        "with open(input_file_path, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader, None)  # Skip the header row\n",
        "\n",
        "    # Shuffle the rows randomly\n",
        "    rows = list(reader)\n",
        "    random.shuffle(rows)\n",
        "\n",
        "    # Compute the number of rows for each split\n",
        "    num_rows = len(rows)\n",
        "    num_train = int(num_rows * train_split)\n",
        "    num_val = int(num_rows * val_split)\n",
        "    num_test = num_rows - num_train - num_val\n",
        "\n",
        "    # Write the rows to the output CSV files\n",
        "    with open(train_csv_file, 'w', newline='') as train_f, \\\n",
        "            open(val_csv_file, 'w', newline='') as val_f, \\\n",
        "            open(test_csv_file, 'w', newline='') as test_f:\n",
        "        train_writer = csv.writer(train_f)\n",
        "        val_writer = csv.writer(val_f)\n",
        "        test_writer = csv.writer(test_f)\n",
        "\n",
        "        # Write the rows to the output CSV files\n",
        "        for i, row in enumerate(rows):\n",
        "            if i < num_train:\n",
        "                train_writer.writerow(row)\n",
        "            elif i < num_train + num_val:\n",
        "                val_writer.writerow(row)\n",
        "            else:\n",
        "                test_writer.writerow(row)\n",
        "                \n",
        "# Remove blank rows from the output CSV files\n",
        "for file_path in [train_csv_file, val_csv_file, test_csv_file]:\n",
        "    with open(file_path, 'r') as f:\n",
        "        rows = list(csv.reader(f))\n",
        "    \n",
        "    with open(file_path, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        for row in rows:\n",
        "            if any(row):\n",
        "                writer.writerow(row)\n",
        "                \n",
        "print(\"Finished!\")\n"
      ],
      "metadata": {
        "id": "7QJTZaXapIaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47322636-1d0d-4398-8095-ea2908dbc4bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check new train, val, and test CSV Files\n",
        "\n",
        " *** START HERE IF DATA IS READY ***"
      ],
      "metadata": {
        "id": "98IgemB03okj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "csv_train_path = \"/content/drive/MyDrive/labeled_images_dataset/train.csv\"  # Replace with the path to your CSV file\n",
        "csv_val_path = \"/content/drive/MyDrive/labeled_images_dataset/val.csv\"  # Replace with the path to your CSV file\n",
        "csv_test_path = \"/content/drive/MyDrive/labeled_images_dataset/test.csv\"  # Replace with the path to your CSV file\n",
        "\n",
        "#Load Data Frame\n",
        "df_train = pd.read_csv(csv_train_path)\n",
        "df_val = pd.read_csv(csv_val_path)\n",
        "df_test = pd.read_csv(csv_test_path)\n",
        "\n",
        "#Print first three values\n",
        "print(df_train.head(3))\n",
        "print(df_val.head(3))\n",
        "print(df_test.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJtE6nGy3qfn",
        "outputId": "bd098f68-c87a-4273-802d-e2990e333cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  filename                                               path  xmin  ymin  \\\n",
            "0   11.jpg  /content/drive/MyDrive/labeled_images_dataset/...   740   520   \n",
            "1   31.jpg  /content/drive/MyDrive/labeled_images_dataset/...   533   420   \n",
            "2   33.jpg  /content/drive/MyDrive/labeled_images_dataset/...   126   369   \n",
            "\n",
            "   xmax  ymax           label  \n",
            "0   783   545           notch  \n",
            "1   578   488   healthy_frond  \n",
            "2   360   534  infested_frond  \n",
            "  filename                                               path  xmin  ymin  \\\n",
            "0   35.jpg  /content/drive/MyDrive/labeled_images_dataset/...   348   591   \n",
            "1   31.jpg  /content/drive/MyDrive/labeled_images_dataset/...   843   835   \n",
            "2   24.jpg  /content/drive/MyDrive/labeled_images_dataset/...   750   104   \n",
            "\n",
            "   xmax  ymax          label  \n",
            "0   400   628          notch  \n",
            "1  1000  1000  infested_tree  \n",
            "2  1000   709   healthy_tree  \n",
            "  filename                                               path  xmin  ymin  \\\n",
            "0   21.jpg  /content/drive/MyDrive/labeled_images_dataset/...   380     1   \n",
            "1   12.jpg  /content/drive/MyDrive/labeled_images_dataset/...   938    41   \n",
            "2   24.jpg  /content/drive/MyDrive/labeled_images_dataset/...    72   644   \n",
            "\n",
            "   xmax  ymax          label  \n",
            "0   809   217  infested_tree  \n",
            "1   973    79          notch  \n",
            "2   183   799  healthy_frond  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define:\n",
        "\n",
        "\n",
        "* Preprocessing\n",
        "* Data Augmentation\n",
        "* Batch Size\n",
        "* Image Size\n",
        "* Generators\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "36rl0zx_BSTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "# Define image preprocessing and data augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Define batch size and image size\n",
        "batch_size = 32\n",
        "img_size = (224, 224)\n",
        "\n",
        "# Load dataframes\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_val = pd.read_csv('val.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "# Define data generators for training, validation, and testing\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=df_train,\n",
        "    x_col='path',\n",
        "    y_col='label',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    dataframe=df_val,\n",
        "    x_col='path',\n",
        "    y_col='label',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=df_test,\n",
        "    x_col='path',\n",
        "    y_col='label',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Catch warning and extract invalid image filenames for train_generator\n",
        "invalid_paths_train = []\n",
        "with warnings.catch_warnings(record=True) as w:\n",
        "    train_generator.next()\n",
        "    for warning in w:\n",
        "        if 'invalid image filename(s)' in str(warning.message):\n",
        "            invalid_paths_train.extend(str(warning.message).split(' ')[-1].split(';')[:-1])\n",
        "print('Invalid image paths for training generator:', invalid_paths_train)\n",
        "\n",
        "# Catch warning and extract invalid image filenames for val_generator\n",
        "invalid_paths_val = []\n",
        "with warnings.catch_warnings(record=True) as w:\n",
        "    val_generator.next()\n",
        "    for warning in w:\n",
        "        if 'invalid image filename(s)' in str(warning.message):\n",
        "            invalid_paths_val.extend(str(warning.message).split(' ')[-1].split(';')[:-1])\n",
        "print('Invalid image paths for validation generator:', invalid_paths_val)\n",
        "\n",
        "# Catch warning and extract invalid image filenames for test_generator\n",
        "invalid_paths_test = []\n",
        "with warnings.catch_warnings(record=True) as w:\n",
        "    test_generator.next()\n",
        "    for warning in w:\n",
        "        if 'invalid image filename(s)' in str(warning.message):\n",
        "            invalid_paths_test.extend(str(warning.message).split(' ')[-1].split(';')[:-1])\n",
        "print('Invalid image paths for test generator:', invalid_paths_test)\n"
      ],
      "metadata": {
        "id": "5-7p0leQmqf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22afa296-98fe-4091-b044-304f70ef8594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6563 validated image filenames belonging to 5 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 1555 invalid image filename(s) in x_col=\"path\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3287 validated image filenames belonging to 5 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 772 invalid image filename(s) in x_col=\"path\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1098 validated image filenames belonging to 5 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 255 invalid image filename(s) in x_col=\"path\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define and Compile the model"
      ],
      "metadata": {
        "id": "ITPIAJDLHlMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ask user whether to use TPU or GPU\n",
        "accelerator = input(\"Are you using a TPU or GPU runtime? Enter 'TPU' or 'GPU': \")\n",
        "\n",
        "if accelerator == 'TPU':\n",
        "  # TPU address\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  # Set the TPU address in the environment variable\n",
        "  os.environ['TPU_NAME'] = tpu_address\n",
        "\n",
        "  # Create a TPUClusterResolver object from the address\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
        "\n",
        "  # Connect to the TPU\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "\n",
        "  # Initialize the TPU system\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "\n",
        "  # Create the strategy using the resolver\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)\n",
        "elif accelerator == 'GPU':\n",
        "  # Create a MirroredStrategy for using the GPU\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "else:\n",
        "  raise ValueError(\"Invalid accelerator. Must be 'TPU' or 'GPU'.\")\n",
        "\n",
        "# Define and compile the model within the strategy scope\n",
        "with strategy.scope():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.applications.ResNet50(\n",
        "          include_top=False, weights='imagenet', input_shape=(224, 224, 3)\n",
        "      ),\n",
        "      tf.keras.layers.GlobalAveragePooling2D(),\n",
        "      tf.keras.layers.Dense(5, activation='softmax')\n",
        "  ])\n",
        "  model.compile(\n",
        "      loss='categorical_crossentropy',\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oONZGAFbHm8U",
        "outputId": "c2d694ef-acfa-434b-8a8f-e001355c4aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are you using a TPU or GPU runtime? Enter 'TPU' or 'GPU': GPU\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training steps"
      ],
      "metadata": {
        "id": "gjLOhponH0NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of training and validation steps per epoch\n",
        "train_steps_per_epoch = train_generator.n // batch_size\n",
        "val_steps_per_epoch = val_generator.n // batch_size\n",
        "\n",
        "# Define the number of epochs for training\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "9lzKbYI3H2U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation"
      ],
      "metadata": {
        "id": "K5lBvN1oIByP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data augmentation for training data\n",
        "train_augmented_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=df_train,\n",
        "    x_col='path',\n",
        "    y_col='label',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__5dH46WIDGl",
        "outputId": "33e358bd-e4a9-454f-fe28-f3bf79078729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6563 validated image filenames belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fit model with augmentation**"
      ],
      "metadata": {
        "id": "Q2yXeuLOIOOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# Fit the model with data augmentation\n",
        "history = None\n",
        "with tqdm(total=epochs, desc='Training') as pbar:\n",
        "    for epoch in range(epochs):\n",
        "        history = model.fit(\n",
        "            train_augmented_generator,\n",
        "            steps_per_epoch=train_steps_per_epoch,\n",
        "            epochs=1,\n",
        "            validation_data=val_generator,\n",
        "            validation_steps=val_steps_per_epoch,\n",
        "            verbose=0\n",
        "        )\n",
        "        pbar.update(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaoj0FIHIP1_",
        "outputId": "5863c0bb-51bc-4982-d8c8-800d5050263e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 10/10 [1:00:29<00:00, 362.95s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save The model"
      ],
      "metadata": {
        "id": "y8iyKDJ6a6nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model architecture and weights\n",
        "model.save('crb_model.h5')"
      ],
      "metadata": {
        "id": "sV1feK0na7sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model architecture and weights\n",
        "model.save('/content/drive/MyDrive/labeled_images_dataset/crb_model.h5')"
      ],
      "metadata": {
        "id": "RHNRKjhLoV3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**START HERE IF LOADING A MODEL**"
      ],
      "metadata": {
        "id": "DNOxfN8yykq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TR-pH6450Ue0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "csv_test_path = \"/content/drive/MyDrive/labeled_images_dataset/test.csv\"  # Replace with the path to your CSV file\n",
        "\n",
        "# Define batch size and image size\n",
        "batch_size = 32\n",
        "img_size = (224, 224)\n",
        "\n",
        "#Load Data Frame\n",
        "df_test = pd.read_csv(csv_test_path)\n",
        "\n",
        "# Define image preprocessing and data augmentation\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=df_test,\n",
        "    x_col='path',\n",
        "    y_col='label',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sBdhuOlynbw",
        "outputId": "301f8ad8-3202-4a13-92a3-a4c2028988fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1098 validated image filenames belonging to 5 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 255 invalid image filename(s) in x_col=\"path\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 1.   Evaluate model on test set\n",
        "# 2.   NP Conversion\n",
        "# 3.   Loss and Accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "KLQrIIxkId7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(123)\n",
        "model = load_model('/content/drive/MyDrive/labeled_images_dataset/crb_model.h5')\n",
        "class_names = ['notch', 'infested_frond', 'infested_tree', 'healthy_frond', 'healthy_tree']\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_generator.reset()\n",
        "y_pred = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
        "print(f'model.predict: {y_pred}')\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Convert predictions from probabilities to class indices\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(f'Probabilities to class indices array: {y_pred}')\n",
        "\n",
        "print(f'Shape of y_pred: {y_pred.shape}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_generator.reset()\n",
        "loss, accuracy = model.evaluate(test_generator, steps=len(test_generator), verbose=2)\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoFJj4QaIf1V",
        "outputId": "ee5793ac-f65a-42b9-af77-a9b83fa3f5db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35/35 [==============================] - 22s 587ms/step\n",
            "model.predict: [[1.0774378e-02 4.2675366e-04 3.2745188e-01 8.5703939e-02 5.7564300e-01]\n",
            " [9.4519460e-01 5.2036025e-02 5.1819981e-04 6.2199874e-04 1.6291504e-03]\n",
            " [7.1029305e-01 4.9777780e-02 8.8216037e-02 2.9705910e-02 1.2200716e-01]\n",
            " ...\n",
            " [9.2529327e-01 7.4056908e-02 8.8848567e-05 1.6270435e-04 3.9822311e-04]\n",
            " [8.3612114e-01 5.1163107e-02 3.5673600e-02 1.8076336e-02 5.8965772e-02]\n",
            " [8.6499131e-01 7.8981072e-02 1.7045557e-02 7.9624103e-03 3.1019645e-02]]\n",
            "Probabilities to class indices array: [4 0 0 ... 0 0 0]\n",
            "Shape of y_pred: (1098,)\n",
            "35/35 - 21s - loss: 1.7775 - accuracy: 0.6129 - 21s/epoch - 586ms/step\n",
            "Test Loss: 1.7774949073791504\n",
            "Test Accuracy: 0.6129326224327087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate classification report and confusion matrix"
      ],
      "metadata": {
        "id": "BXN9DNtGIiES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "class_names = ['notch', 'infested_frond', 'infested_tree', 'healthy_frond', 'healthy_tree']\n",
        "\n",
        "# Reshape y_pred\n",
        "y_pred_reshaped = y_pred[:, np.newaxis]\n",
        "\n",
        "# Generate the classification report and confusion matrix\n",
        "report = classification_report(y_true, np.argmax(y_pred_reshaped, axis=1), target_names=class_names)\n",
        "print(report)\n",
        "\n",
        "cm = confusion_matrix(y_true, np.argmax(y_pred_reshaped, axis=1))\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Actual')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(class_names)\n",
        "ax.yaxis.set_ticklabels(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "vBMNx_rUIoIX",
        "outputId": "695b2193-1c81-4125-e8d5-c8b54cfd0346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AxisError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-00b2c0926ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_pred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Generate the classification report and confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \"\"\"\n\u001b[0;32m-> 1195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# Generate the AUC-ROC curve\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "num_classes = 5 # notch, infested_frond, infested_tree, healthy_frond, healthy_tree\n",
        "for i in tqdm(range(num_classes), desc='Computing ROC curve'):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])"
      ],
      "metadata": {
        "id": "RiQkrSCeKZbk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "5e22946a-43a0-433a-d0d4-d958c7f54f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing ROC curve:   0%|          | 0/5 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-07ef477601e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;31m# notch, infested_frond, infested_tree, healthy_frond, healthy_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Computing ROC curve'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m     \"\"\"\n\u001b[0;32m--> 962\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[1;32m    963\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    727\u001b[0m     \"\"\"\n\u001b[1;32m    728\u001b[0m     \u001b[0;31m# Check to make sure y_true is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0;34m\"Expected array-like (array or non-string sequence), got %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Expected array-like (array or non-string sequence), got False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(label_binarize(y_true, classes=range(num_classes)).ravel(), y_pred.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
      ],
      "metadata": {
        "id": "MuZ1_DyJKb5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curve for each class\n",
        "plt.figure()\n",
        "lw = 2\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red'])\n",
        "for i, color in zip(range(num_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw, label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
        "    \n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', lw=lw, linestyle='--',\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3n6PxSBbKgrD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}